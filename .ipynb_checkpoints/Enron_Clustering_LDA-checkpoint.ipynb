{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Latent Dirichlet Allocation Analysis\n",
    "<hr>\n",
    "**Author: ** *Gilberto Diaz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('enron_lda').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('./user_and_emails.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   user|          email_body|\n",
      "+-------+--------------------+\n",
      "|allen-p|Here is our forec...|\n",
      "|allen-p|Traveling to have...|\n",
      "|allen-p|test successful  ...|\n",
      "|allen-p|Randy    Can you ...|\n",
      "|allen-p|Let s shoot for T...|\n",
      "|allen-p|Greg    How about...|\n",
      "|allen-p|Please cc the fol...|\n",
      "|allen-p|any morning betwe...|\n",
      "|allen-p|1  login   pallen...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Mr  Buckner    Fo...|\n",
      "|allen-p|Lucy    Here are ...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Dave     Here are...|\n",
      "|allen-p|Paula    35 milli...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Tim   mike grigsb...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|                 ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StopWordsRemover, \n",
    "                                Tokenizer, \n",
    "                                CountVectorizer, \n",
    "                                RegexTokenizer,\n",
    "                                IDF)\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   user|          email_body|\n",
      "+-------+--------------------+\n",
      "|allen-p|Here is our forec...|\n",
      "|allen-p|Traveling to have...|\n",
      "|allen-p|test successful  ...|\n",
      "|allen-p|Randy    Can you ...|\n",
      "|allen-p|Let s shoot for T...|\n",
      "|allen-p|Greg    How about...|\n",
      "|allen-p|Please cc the fol...|\n",
      "|allen-p|any morning betwe...|\n",
      "|allen-p|1  login   pallen...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Mr  Buckner    Fo...|\n",
      "|allen-p|Lucy    Here are ...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Dave     Here are...|\n",
      "|allen-p|Paula    35 milli...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|Tim   mike grigsb...|\n",
      "|allen-p|                 ...|\n",
      "|allen-p|                 ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_with_id = data.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+\n",
      "|   user|          email_body| id|\n",
      "+-------+--------------------+---+\n",
      "|allen-p|Here is our forec...|  0|\n",
      "|allen-p|Traveling to have...|  1|\n",
      "|allen-p|test successful  ...|  2|\n",
      "|allen-p|Randy    Can you ...|  3|\n",
      "|allen-p|Let s shoot for T...|  4|\n",
      "|allen-p|Greg    How about...|  5|\n",
      "|allen-p|Please cc the fol...|  6|\n",
      "|allen-p|any morning betwe...|  7|\n",
      "|allen-p|1  login   pallen...|  8|\n",
      "|allen-p|                 ...|  9|\n",
      "|allen-p|Mr  Buckner    Fo...| 10|\n",
      "|allen-p|Lucy    Here are ...| 11|\n",
      "|allen-p|                 ...| 12|\n",
      "|allen-p|                 ...| 13|\n",
      "|allen-p|Dave     Here are...| 14|\n",
      "|allen-p|Paula    35 milli...| 15|\n",
      "|allen-p|                 ...| 16|\n",
      "|allen-p|Tim   mike grigsb...| 17|\n",
      "|allen-p|                 ...| 18|\n",
      "|allen-p|                 ...| 19|\n",
      "+-------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_id.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and removing empty tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='email_body', outputCol='tokens', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_tokenized = regex_tokenizer.transform(data_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+--------------------+\n",
      "|   user|          email_body| id|              tokens|\n",
      "+-------+--------------------+---+--------------------+\n",
      "|allen-p|Here is our forec...|  0|[here, is, our, f...|\n",
      "|allen-p|Traveling to have...|  1|[traveling, to, h...|\n",
      "|allen-p|test successful  ...|  2|[test, successful...|\n",
      "|allen-p|Randy    Can you ...|  3|[randy, can, you,...|\n",
      "|allen-p|Let s shoot for T...|  4|[let, s, shoot, f...|\n",
      "|allen-p|Greg    How about...|  5|[greg, how, about...|\n",
      "|allen-p|Please cc the fol...|  6|[please, cc, the,...|\n",
      "|allen-p|any morning betwe...|  7|[any, morning, be...|\n",
      "|allen-p|1  login   pallen...|  8|[1, login, pallen...|\n",
      "|allen-p|                 ...|  9|[forwarded, by, p...|\n",
      "|allen-p|Mr  Buckner    Fo...| 10|[mr, buckner, for...|\n",
      "|allen-p|Lucy    Here are ...| 11|[lucy, here, are,...|\n",
      "|allen-p|                 ...| 12|[forwarded, by, p...|\n",
      "|allen-p|                 ...| 13|[forwarded, by, p...|\n",
      "|allen-p|Dave     Here are...| 14|[dave, here, are,...|\n",
      "|allen-p|Paula    35 milli...| 15|[paula, 35, milli...|\n",
      "|allen-p|                 ...| 16|[forwarded, by, p...|\n",
      "|allen-p|Tim   mike grigsb...| 17|[tim, mike, grigs...|\n",
      "|allen-p|                 ...| 18|[forwarded, by, p...|\n",
      "|allen-p|                 ...| 19|[forwarded, by, p...|\n",
      "+-------+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover()\n",
    "new_stop_words = [\n",
    "    '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '0', '1', \n",
    "    '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '15', \n",
    "    '16', '17', '18', '19','20', '25', 'www', 'com', 'pm', '853', 'click',\n",
    "    'mail', 'e', 'http', 'na', 'ees', 'cc', 'hou', 'etc', '30', 'll', '35',\n",
    "    'a','cannot','into','our','thus','about','co','is','ours','to','above',\n",
    "    'could','it','ourselves','together','across','down','its','out','too',\n",
    "    'after','during','itself','over','toward','afterwards','each','last',\n",
    "    'own', 'towards','again','eg','latter','per','under','against','either',\n",
    "    'latterly', 'perhaps','until','all','else','least','rather','up','almost',\n",
    "    'elsewhere', 'less','same','upon','alone','enough','seem','us', 'said',\n",
    "    'along','etc', 'many','seemed','very','already','even','may','seeming','via',\n",
    "    'also','ever', 'me','seems','was','although','every','meanwhile','several',\n",
    "    'always', 'everyone','might','she','well','among','everything','more','should',\n",
    "    'were', 'amongst','everywhere','moreover','since','what','an','except','most',\n",
    "    'whatever','and','few','mostly','some','when','another','first','much', \n",
    "    'somehow','whence','any','for','must','someone','whenever','anyhow', 'mr', \n",
    "    'my','something','where','anyone','formerly','myself','sometime', 'gif',\n",
    "    'whereafter','anything','from','namely','sometimes','whereas','anywhere', \n",
    "    'further','neither','somewhere','whereby','are','had','never','still', 'let',\n",
    "    'wherein','around','has','nevertheless','such','whereupon','as','have', 'ip',\n",
    "    'next','than','wherever','at','he','no','that','whether','be','hence',\n",
    "    'nobody','the','whither','became','her','none','their','which','because', 'send',\n",
    "    'here','noone','them','while','become','hereafter','nor','themselves','who',\n",
    "    'becomes','hereby','not','then','whoever','becoming','herein','nothing',\n",
    "    'thence','whole','been','hereupon','now','there','whom','before','hers', 'ski',\n",
    "    'nowhere','thereafter','whose','beforehand','herself','of','thereby','why', \n",
    "    'especially', 'image', 're', 'we', 'so', 'static', 'width',\n",
    "    'behind','him','off','therefore','will','being','himself','often','therein', \n",
    "    'with','below','his','on','thereupon','within','beside','how','once', 'try', \n",
    "    'these','without','besides','however','one','they','would','between','i', 'far',\n",
    "    'only','this','yet','beyond','ie','onto','those','you','both','if','or', 'get',\n",
    "    'though','your','but','in','other','through','yours','by','inc','others', 'suggest', \n",
    "    'take', 'throughout','yourself','can','indeed','otherwise','thru','yourselves', \n",
    "    'login', 'please', 'forwarded', 'pw', 'k', '-', '+', '|', ' ', 'go', 'takes', \n",
    "    'td', 'font', 'br', 'b', 'tr', 'm', 'align', 'net', '3d', '2001', 'new', \n",
    "    'said', '11', 'ect', '2000', 'sent', 'know', 'dbcaps97data',\n",
    "    '12', 'need', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '12', 'aol',\n",
    "    '2002', 'mailto', '713', 'error', 'nbsp', 'et', \n",
    "    'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
    "    ]\n",
    "\n",
    "all_stop_words = stop_words.getStopWords() + new_stop_words\n",
    "stop_words_set = set(all_stop_words)\n",
    "stop_words_set = list(stop_words_set)\n",
    "\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='words', stopWords=stop_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = remover.transform(regex_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(inputCol='words', outputCol='vectors') # to user with IDF\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_model = cv.fit(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = count_vectorizer_model.transform(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = count_vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idf = IDF(inputCol='vectors', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idf_model = idf.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale_data = idf_model.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|   user|          email_body| id|              tokens|               words|            features|\n",
      "+-------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|allen-p|Here is our forec...|  0|[here, is, our, f...|          [forecast]|(262144,[1921],[1...|\n",
      "|allen-p|Traveling to have...|  1|[traveling, to, h...|[traveling, busin...|(262144,[6,16,29,...|\n",
      "|allen-p|test successful  ...|  2|[test, successful...|[test, successful...|(262144,[60,959,1...|\n",
      "|allen-p|Randy    Can you ...|  3|[randy, can, you,...|[randy, schedule,...|(262144,[31,113,1...|\n",
      "|allen-p|Let s shoot for T...|  4|[let, s, shoot, f...|[shoot, tuesday, 45]|(262144,[82,407,6...|\n",
      "|allen-p|Greg    How about...|  5|[greg, how, about...|[greg, tuesday, t...|(262144,[77,82,46...|\n",
      "|allen-p|Please cc the fol...|  6|[please, cc, the,...|[following, distr...|(262144,[0,25,53,...|\n",
      "|allen-p|any morning betwe...|  7|[any, morning, be...|           [morning]|(262144,[278],[1.0])|\n",
      "|allen-p|1  login   pallen...|  8|[1, login, pallen...|[pallen, ke9davis...|(262144,[8,47,153...|\n",
      "|allen-p|                 ...|  9|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,2,3,...|\n",
      "|allen-p|Mr  Buckner    Fo...| 10|[mr, buckner, for...|[buckner, deliver...|(262144,[0,3,7,44...|\n",
      "|allen-p|Lucy    Here are ...| 11|[lucy, here, are,...|[lucy, rentrolls,...|(262144,[285,573,...|\n",
      "|allen-p|                 ...| 12|[forwarded, by, p...|[phillip, allen, ...|(262144,[1,6,7,10...|\n",
      "|allen-p|                 ...| 13|[forwarded, by, p...|[phillip, allen, ...|(262144,[1,6,7,10...|\n",
      "|allen-p|Dave     Here are...| 14|[dave, here, are,...|[dave, names, wes...|(262144,[296,323,...|\n",
      "|allen-p|Paula    35 milli...| 15|[paula, 35, milli...|[paula, million, ...|(262144,[45,940,1...|\n",
      "|allen-p|                 ...| 16|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,4,6,...|\n",
      "|allen-p|Tim   mike grigsb...| 17|[tim, mike, grigs...|[tim, mike, grigs...|(262144,[2,33,83,...|\n",
      "|allen-p|                 ...| 18|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,11,3...|\n",
      "|allen-p|                 ...| 19|[forwarded, by, p...|[phillip, allen, ...|(262144,[0,1,4,27...|\n",
      "+-------+--------------------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rescale_data.show()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda_model.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = model.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[41, 45, 8, 493, ...|[0.00505194404606...|\n",
      "|    1|[132, 358, 390, 1...|[0.00891474809031...|\n",
      "|    2|[0, 1, 4, 5, 9, 1...|[0.04050886364638...|\n",
      "|    3|[0, 2, 3, 8, 14, ...|[0.00841943508153...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_words = topics_rdd\\\n",
    "    .map(lambda row: row['termIndices'])\\\n",
    "    .map(lambda inx_list: [vocab[idx] for idx in inx_list])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "===============\n",
      "000\n",
      "million\n",
      "company\n",
      "omni\n",
      "gas\n",
      "50\n",
      "market\n",
      "firm\n",
      "31\n",
      "database\n",
      "\n",
      "\n",
      "Topic: 2\n",
      "===============\n",
      "size\n",
      "href\n",
      "fantasy\n",
      "final\n",
      "class\n",
      "table\n",
      "face\n",
      "sportsline\n",
      "arial\n",
      "updated\n",
      "\n",
      "\n",
      "Topic: 3\n",
      "===============\n",
      "enron\n",
      "subject\n",
      "corp\n",
      "message\n",
      "thanks\n",
      "original\n",
      "time\n",
      "call\n",
      "attached\n",
      "mark\n",
      "\n",
      "\n",
      "Topic: 4\n",
      "===============\n",
      "enron\n",
      "power\n",
      "energy\n",
      "company\n",
      "state\n",
      "california\n",
      "market\n",
      "gas\n",
      "year\n",
      "business\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(topics_words):\n",
    "    print(f'Topic: {idx+1}')\n",
    "    print('===============')\n",
    "    for word in topic:\n",
    "        print(word)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I played around with different setups trying to get coherent topics. With the IDF packages, words in topics where all over the place. This current setup gave me the most coherent topics. I'm open to learn how to fine tune passing different parameters.\n",
    "\n",
    "### Topic: 4\n",
    "I will classify this topic as very corporate conversations.\n",
    "\n",
    "### Topic: 3\n",
    "I will classify this topic as meeting conversations since have words such as call, time, attached, and mark.\n",
    "\n",
    "### Topic: 2\n",
    "I don't have any classification for this topic.\n",
    "\n",
    "### Topic: 1\n",
    "I will classify this topic as stock market\n",
    "\n",
    "\n",
    "## TODO:\n",
    "<hr>\n",
    "- Gain more knowledge about the domain for fine tuning stop words.\n",
    "- Find resources to learn how to fine tune by tweaking function's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
